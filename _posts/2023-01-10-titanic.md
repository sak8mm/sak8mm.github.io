---
layout: post
title: Predicting survival
image: "/posts/titanic.png"
tags: [Titanic, Machine Learning, Regression, Python]
---

In this setup I am going to walk through a famous Kaggle website dataset predicting survival from the titanic shipwreck. This is a popular dataset from Kaggle.com that is used to practice machine learning (ML) and other techniques.

# Table of contents

- [00. Project Overview](#overview-main)
    - [Context](#overview-context)
    - [Actions](#overview-actions)
    - [Results](#overview-results)
    - [Growth/Next Steps](#overview-growth)
- [01. Data Preprocessing](#data-preprocessing)
- [02. Modeling Overview](#modeling-overview)
- [03. Pre GridSearchCV modeling](#pre-gscv-title)
- [04. Post GridSearchCV modeling](#post-gscv-title)
- [05. Modeling Summary](#modeling-summary)
- [06. Growth & Next Steps](#growth-next-steps)

___

# Project Overview  <a name="overview-main"></a>

### Context <a name="overview-context"></a>

Using the data provided with all its information, we will use machine learning to create a model that predicts which passengers survived the Titanic shipwreck. This challenge specifically asks, “what sort of people were more likely to survive?” given the data available.
Since we were tasked with building a model on whether passengers survived or not, it is what’s called a classification problem. Meaning, did a person survive (1 = person survived) or did a person NOT survive (0 = person did not survive).
To achieve this, we looked to build out a predictive classification model that will find relationships between passenger data and *survived* for those passengers who were tagged as *survived*. In this case we know the result in the data for each instance (row) because we are given instances where the data states survived or not. This means our model is a *supervised* model.

<br>
<br>
### Actions <a name="overview-actions"></a>

Since we have the data given to use and before we started the data analysis, we needed to decide on the classification model(s), and how those models would be evaluated. 
For the metric in evaluating the models we used what is called the *F1 score*. There are a couple of different metrics to use such as accuracy, recall, precision, AUC-ROC, etc., but the F1 score is the right balance for the classification model metric we are looking for in this case.
As we are predicting a classification binary output (survived = 1 / did not survive = 0), we tested three classification modeling approaches, namely:

* Random Forest 
* Logistic Regression
* Support Vector Machine (SVM)
<br>
<br>

###Results <a name="overview-results"></a>

Testing found Random Forest had the highest predictive F1 score pre and post grid search cross validation (GridSearchCV).

<br>
**Metric 1: Pre- GridSearchCV F1 score (Test Set)**

* Random Forest = 0.727
* Logistic Regression = 0.660
* SVM = 0.683

<br>
**Metric 2: Post-g GridSearchCV F1 score (Test Set)**

* Random Forest = 0.780
* Logistic Regression = 0.661
* SVM = 0.694

<br>
<br>
### Growth/Next Steps <a name="overview-growth"></a>

While the F1 score was relatively high - other modeling approaches could be tested, especially those somewhat similar to Random Forest, for example XGBoost, LightGBM to see if a better F1 score  could be gained.

From a data point of view, further variables could be collected, and further feature engineering could be undertaken to ensure that we have as much useful information available for predicting survival.
<br>
<br>
___

# Data Preprocessing  <a name="data-preprocessing"></a>

We first look at the data to see what we have missing and decide how we will address the missing data. Also, address possible feature engineering with certain data columns given in the data set.

```python

# import required packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn import svm
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, f1_score
from sklearn.model_selection import GridSearchCV
from sklearn.inspection import permutation_importance

# import the data to pandas dataframe and lowercase the columns, then mix the data.
train = pd.read_csv('train.csv')
train.columns = train.columns.str.lower() 

train = train.sample(frac=1).reset_index(drop=True)

train_null = train.isnull().sum()
```
<br> 

Summary of missing data for columns: 

|       Column      |  Count |
|:------------|----:|
| passengerid |   0 |
| survived    |   0 |
| pclass      |   0 |
| name        |   0 |
| sex         |   0 |
| age         | 177 |
| sibsp       |   0 |
| parch       |   0 |
| ticket      |   0 |
| fare        |   0 |
| cabin       | 687 |
| embarked    |   2 |
<br>

We have three columns with missing data: age, cabin, and embarked. To address this, we used the following code to fill-in the missing data for *embarked* and *age*. We will do something slightly different with cabin because there are a lot of missing values. With that many missing values, some people might drop that column, but missing values can be telling as well for models so we will use it.

<br>

```python

train['embarked'] = train.embarked.fillna(train.embarked.value_counts().index[0])
train['age'] = train.age.fillna(train.age.median())

```
<br>

Since I have seen the movie Titanic (the old and new versions) I know the survivor rate should be higher with women and children. I also believe class type on the ship will play a factor. I checked my assumptions to see if my memory was in the ballpark.
<br>
<br>

```python
crosstbl = (pd.crosstab([train.survived, train.sex], train.pclass, values = train.sex.count(), aggfunc = 'count', normalize = 'columns')
                        .mul(100)
                        .round(2))

sns.heatmap(crosstbl, annot=True, fmt='.2f', linewidth=1, cmap='crest')
```
<br>

The code above gives us the following heatmap by *sex* and *pclass* according to survival in percentages (normalized).
<br>

![alt text](/img/posts/sex_pclass.png "Heatmap of Sex with Pclass")

<br>

According to the heatmap, most of the deaths came from the third class in the column *pclass* and were mostly *male*.

<br>

Since sex and plcass appears to play a role in survival I was curious about age and fare so I used the following code to show a scatter plot.
<br>
<br>

```python
graph = sns.FacetGrid(train, hue='survived', col='sex', palette="Set1")
graph.map(plt.scatter, 'fare', 'age', edgecolor='k').add_legend()
plt.subplots_adjust(top=0.8)
graph.fig.suptitle('Survival by Gender, Age, and Fare')
```
<br>
Results in the following graph:

![alt text](/img/posts/survival_by_gender_age_fare.png "Survival by Gender, Age, and Fare")

<br>

Next in our data preprocessing I created new features (columns) based on some features I thought would be useful in our model. We are missing a lot of data in the *cabin* feature so I used the top 4 values in that category to create new features. 'n' will be one of those for null value. I did similar work with the feature *ticket* using its length as well as starting letter/number for the ticket. Then created a new feature called *family_size*.
<br>
<br>

```python

# creates 4 new columns based on top 4 cabin letters.
train['cabin_letter'] = train.cabin.astype(str).str[0]
top_4_cabin_letter = [x for x in train.cabin_letter.value_counts().sort_values(ascending=False).head(4).index]

for label in top_4_cabin_letter:
    train[label] = np.where(train['cabin_letter']==label, 1, 0)
     
# creates family size categories based on number of people.
train['family'] = train['sibsp'] + train['parch'] + 1

condlist_train = [train['family'] > 6, train['family'] > 3,train['family'] > 1]
outcome = ['large', 'medium', 'small' ]

train['family_size'] = np.select(condlist_train, outcome, default = 'single')

# creates 4 new columns based on the top 4 ticket lengths.
train['ticket_length'] = train['ticket'].str.len()
top_4_ticket = [x for x in train.ticket_length.value_counts().sort_values(ascending=False).head(4).index]

for label in top_4_ticket:
    train[label] = np.where(train['ticket_length']==label, 1, 0)

# creates 4 new columns based on top 4 ticket starting number/letter.
train['ticket_start'] = train['ticket'].str[0]
top_4_ticket_start = [x for x in train.ticket.str[0].value_counts().sort_values(ascending=False).head(4).index]

for label in top_4_ticket_start:
    train[label] = np.where(train['ticket_start']==label, 1, 0)

# make new columns string type.
train.rename(columns={6: "6", 5: "5", 4: "4", 8: "8", 3: "3", 2: "2", 1: "1"}, inplace=True)

# drop columns not used for the model.
train.drop(['passengerid', 'name', 'sibsp', 'parch', 'ticket', 'cabin', 'cabin_letter', 'family', 'ticket_length', 'ticket_start'], axis = 1, inplace=True)

```
<br>

Lastly, we had some final columns to clean up for our classification models. Those columns being *age*, *fare*, *sex*, *embarked*, and *family_size*. For age and fare, we  normalized the continuous data these columns have on a 0 to 1 scale. For the others, we use pandas *get_dummies* function that creates dummy columns (discrete variables) for these categorical features. All that was done with the following code:

<br> 

```python

scaler = MinMaxScaler()

train[['age', 'fare']] = scaler.fit_transform(train[['age', 'fare']])

train = pd.get_dummies(data=train, columns=['embarked', 'family_size', 'sex'], drop_first=True)

```
<br> 

___


# Modeling Overview <a name="data-preprocessing"></a>

We will build classification models using the features selected and created to see which model yields the best results. As well as see which features are profoundly moving our model in terms of the metric used.

As we are predicting a classification output (binary based on survived), we tested three classification models:
<br>
<br>
* Random Forest 
* Logistic Regression
* Support Vector Machine (SVM)


<br>

We ran the classification models pre and post GridSearchCV. In the pre work we just used the classification models themselves without setting any parameters with the models. For the post, we used the grid search feature to search through a set of parameters for the best model outcome given a list of parameters to use.
<br>
<br>
___

# Pre GridSearchCV modeling <a name="modeling-overview"></a>


To set our models we need to isolate our X and y variables with X being all of our features and y being the dependent variable of *survived*. Then we split the data into train and test sets so we can evaluate our models to see how they fared. The code does all that in the following:


```python

# isolate dependent and independent variables.
X = train.drop(['survived'], axis=1)
y = train['survived']

# split data for model grading.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)

# create the models.
models = [LogisticRegression(random_state=42), svm.SVC(random_state=42), RandomForestClassifier(random_state=42)]
labels = ['lg_clf', 'sv_clf', 'rf_clf']

# cycle through the models fitting and predicting with the variables.
model_dic = dict(zip(labels, models))

model_dic_fit = {}

for k, v in model_dic.items():
    model_dic_fit[k] = model_dic[k].fit(X_train, y_train)
    
    
model_dic_pred = {}

for k, v in model_dic_fit.items():
    model_dic_pred[k] = model_dic_fit[k].predict(X_test)
    
    
```
<br>

The next piece of code actually test how well our classification models did by storing our F1 scores in a python dictionary


```python

f1s = {}

for k, v in model_dic_pred.items():
    f1s[k] = f1_score(y_test, model_dic_pred[k])
    
```
<br>

At last, the results from the classification models are as follows:

* Random Forest = 0.727
* Logistic Regression = 0.660
* SVM = 0.683

Print out of dictionary looks like this: 
{'lg_clf': 0.6608695652173914, 'sv_clf': 0.6833333333333333, 'rf_clf': 0.7272727272727272}

So, it looks like the *Random Forest* model is the best so far, but now let’s see what happens when we use the GridSearchCV approach.
<br>
<br>
___

# Post GridSearchCV modeling <a name="post-gscv-title"></a>

Now for the use of GridSearchCV we set some parameters based on the classification models and then ran the GridSearchCV to see what the best results are for each classification. Once those are known, we apply those to our models and then retrain the model with those parameters.

The following code works through that process:


```python
# building the parameters for each model.
param_grid_rf = {
            'n_estimators': [100, 200, 500, 1000, 1500], 
            'max_features': ['auto', 'sqrt', 'log2'],
            'max_depth': [4, 6, 8, 10],
            'criterion': ['gini', 'entropy']
                }

param_grid_lg = {
              'penalty': ['l1', 'l2', 'none', 'elasticent'],
              'C': [0.001, 0.01, 0.1, 1, 10, 100], 
              'solver': ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga']
                } 

param_grid_sv = {'C': [0.1, 1, 10, 100, 1000], 
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ['rbf']} 

# applying the GridSearchCV to our models for best parameters.
gscv_rf = GridSearchCV(estimator = model_dic['rf_clf'], param_grid = param_grid_rf, n_jobs=-1, cv = 5)
gscv_lg = GridSearchCV(estimator = model_dic['lg_clf'], param_grid = param_grid_lg, n_jobs=-1, cv = 5)
gscv_sv = GridSearchCV(estimator = model_dic['sv_clf'], param_grid = param_grid_sv, n_jobs=-1, cv = 5)


import warnings
with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    gscv_rf.fit(X_train, y_train)
    gscv_lg.fit(X_train, y_train)
    gscv_sv.fit(X_train, y_train)

# gives best parameters to use.    
gsrf_bp = gscv_rf.best_params_
gslg_bp = gscv_lg.best_params_
gssv_bp = gscv_sv.best_params_

print(gsrf_bp)
print(gslg_bp)
print(gssv_bp)

```
<br>

Best parameters for each model are as follows:

Random Forest:
{'criterion': 'gini', 'max_depth': 8, 'max_features': 'auto', 'n_estimators': 200}

Logistic Regression:
{'C': 0.001, 'penalty': 'none', 'solver': 'lbfgs'}

SVM:
{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}


Now when we apply these new parameters and run the following code we should see an increase in our F1 score. Let's see what happens.

```python
rf_clf_bp = RandomForestClassifier(criterion=gsrf_bp['criterion'], max_depth=gsrf_bp['max_depth'], max_features=gsrf_bp['max_features'], 
                                   n_estimators=gsrf_bp['n_estimators'], random_state=42)

lg_clf_bp = LogisticRegression(C = gslg_bp['C'], penalty = gslg_bp['penalty'], solver = gslg_bp['solver'], random_state=42)

sv_clf_bp = svm.SVC(C = gssv_bp['C'], gamma = gssv_bp['gamma'], kernel = gssv_bp['kernel'], random_state=42)

models_bp = [lg_clf_bp, sv_clf_bp, rf_clf_bp]
labels_bp = ['lg', 'sv', 'rf']

model_dic_bp = dict(zip(labels_bp, models_bp))

model_dic_bp_fit = {}

for k, v in model_dic_bp.items():
    model_dic_bp_fit[k] = model_dic_bp[k].fit(X_train, y_train)
    
model_dic_pred_bp = {}

for k, v in model_dic_bp_fit.items():
    model_dic_pred_bp[k] = model_dic_bp_fit[k].predict(X_test)
    

f1s_bp = {}

for k, v in model_dic_pred_bp.items():
    f1s_bp[k] = f1_score(y_test, model_dic_pred_bp[k])
    

print(f1s_bp)
```
<br>

The output from our print(f1s_bp) is the following:

* Random Forest = 0.780
* Logistic Regression = 0.661
* SVM = 0.694

Print out of dictionary looks like this: 
{'lg': 0.6611570247933884, 'sv': 0.6942148760330579, 'rf': 0.7804878048780488}

___

# Modeling Summary  <a name="modeling-summary"></a>

It looks like the Random Forest Classification model is the best because it yields the highest F1 score with 78%. Next we are going to look at the *feature importance* and *permutation importance* of the Random Forest Classifier. This will give us an idea of what features played a role in our model scoring.

First, what is the difference between *feature importance* and *permutation importance*?

Let's say you want to know which toys are the most important to you, and you have a bunch of toys to choose from.

*Feature importance* is figuring out which toys you like the most based on what you know about them. You might like a toy that is colorful, has lots of different parts, or makes noise. You can look at each toy and decide which features make it more important to you than the others.

*Permutation importance* is like figuring out which toys you like the most by actually playing with them. You can take each toy and play with it for a while to see how much you like it. Then you can put it back and pick another toy to play with. You can do this for all the toys and figure out which ones you liked the most based on how much you enjoyed playing with them.

In the same way, feature importance is a way to figure out which features are the most important in a machine learning model based on what we know about them. Permutation importance is a way to figure out which features are the most important by actually testing how much the model's predictions change when we remove each feature.



```python
# feature importance
feature_importance = pd.DataFrame(model_dic_bp['rf'].feature_importances_)
feature_names = pd.DataFrame(X.columns)
feature_importance_summary = pd.concat([feature_names, feature_importance], axis=1)
feature_importance_summary.columns = ['input_variable', 'feature_importance']
feature_importance_summary.sort_values(by = 'feature_importance', inplace = True)

plt.barh(feature_importance_summary['input_variable'], feature_importance_summary['feature_importance'])
plt.title('feature importance of random forest')
plt.xlabel('feature importance')
plt.tight_layout()
plt.show()

# permutation importance
result = permutation_importance(model_dic_bp['rf'], X_test, y_test, n_repeats = 10, random_state=42)

permutation_importance = pd.DataFrame(result['importances_mean'])
feature_names = pd.DataFrame(X.columns)
permutation_importance_summary = pd.concat([feature_names, permutation_importance], axis=1)
permutation_importance_summary.columns = ['input_variable', 'permutation_importance']
permutation_importance_summary.sort_values(by = 'permutation_importance', inplace = True)

plt.barh(permutation_importance_summary['input_variable'], permutation_importance_summary['permutation_importance'])
plt.title('permutation importance of random forest')
plt.xlabel('permutation importance')
plt.tight_layout()
plt.show()
```
<br>

![alt text](/img/posts/FeatureImportanceRF.png "Random Forest Feature Importance")

<br>

![alt text](/img/posts/permutationImportanceRF.png "Random Forest Permutation Importance")

<br>

Interesting looking at the differences in these graphs. In both graphs the top 4 features were *sex*, *age*, *pclass*, and *fare*. Sex being the dominant feature which is not surprising to anyone who saw the movie. 

<br>


___
# Growth & Next Steps <a name="growth-next-steps"></a>

Even with a little bit of feature engineering, model parameter tuning we achieved a big lift in our F1 score. However, other modeling approaches could be tested, especially those somewhat similar to Random Forest, for example XGBoost, LightGBM to see if a better F1 score  could be gained.

From a data point of view, further variables could be collected, and further feature engineering could be undertaken to ensure that we have as much useful information available for predicting survival. We could also utilize what's called *recursive feature elimination cross validation (RFECV)*.

RFECV is a technique used in machine learning to select the most relevant features for a given model. It combines recursive feature elimination (RFE) and cross-validation to evaluate and rank the importance of each feature in a dataset.

RFE is a feature selection method that recursively removes the least important feature(s) from the dataset and trains the model on the remaining features. This process is repeated until a specified number of features is reached or until a performance metric is optimized.

In RFECV, the RFE process is performed multiple times, each time using a different subset of the data for cross-validation. The performance of the model is evaluated using a performance metric such as accuracy or mean squared error, and the feature ranking is updated based on the cross-validation results.

It can be computationally intensive so again, it depends on the parameters and how much data we are working with.



