---
layout: post
title: 2019 DC crime incidents
image: "/posts/dc_pic.png"
tags: [Python, data analysis, crime, DC]
---

In this post I am going to walk through a data analysis process I did from a opendata.dc.gov site for 2019 data located at https://opendata.dc.gov/datasets/DCGIS::crime-incidents-in-2019/about. I actually downloaded this csv file in July of 2021 for this walkthough.

---
I am going to first import some python packages I need for this:

```python
import pandas as pd
import seaborn as sns
%matplotlib inline
import matplotlib.pyplot as plt
```
The pandas package is to handle the data in a tabular fashion and makes the data easier to manipulate for use to look at it in different ways. The seaborn and matplotlib are used to visualize the data when we have it set up in ways I want to take a look at it. 

Ok, lets get into the thought process on how we want to look at this and see what the data set has to offer us. To do this, I use pandas because it will put the data into what's called a dataframe. The easiest way to explain this is the information is set up in a tabular fashion similar to information on an excel spreadsheet. Everyone knows what an excel spreadsheet is right?

```python
df = pd.read_csv('Crime_Incidents_in_2019.csv')
df.columns = df.columns.str.strip().str.lower() # I prefer to work with lower case for ease as well stripping potential unneccesary spaces in the colums.
```
The following function is one I use when I set up my dataframe(s). I actually got this function from a class I attended at the University of Virginia for Business Analytics. I use to this because it works for what I am looking for and my thought is why invent the wheel if this bad boy gives me the initial insights on my next steps to pursue.

```python
def summarize_dataframe(df):
    """Gives break down of dataframe for a holistic look"""
    missing_values = pd.concat([pd.DataFrame(df.columns, columns=['Variable Name']), 
                      pd.DataFrame(df.dtypes.values.reshape([-1,1]), columns=['Data Type']),
                      pd.DataFrame(df.isnull().sum().values, columns=['Missing Values']), 
                      pd.DataFrame([df[name].nunique() for name in df.columns], columns=['Unique Values'])], 
                     axis=1).set_index('Variable Name')
    return pd.concat([missing_values, df.describe(include='all').transpose()], axis=1).fillna("")

summarize_dataframe(df) # Runs the function directly above.
print(df.shape) # Gives the total rows and columns in the dataframe.
```
Once we run the code we get the following information to assess which I will just show the first three columns since that is what we are interested in:

Rows in the df = 33,922

Columns in the df = 25 

Summary table:

|    Variables   |  Data Type   |   Missing Values |
|:--------------------:|:-----------:|:----------------:|
| x                    | float64     |                0 |
| y                    | float64     |                0 |
| ccn                  | int64       |                0 |
| report_dat           | object      |                0 |
| shift                | object      |                0 |
| method               | object      |                0 |
| offense              | object      |                0 |
| block                | object      |                0 |
| xblock               | float64     |                0 |
| yblock               | float64     |                0 |
| ward                 | float64     |             1036 |
| anc                  | object      |             1036 |
| district             | float64     |                2 |
| psa                  | float64     |                2 |
| neighborhood_cluster | object      |             1036 |
| block_group          | object      |             1037 |
| census_tract         | float64     |             1037 |
| voting_precinct      | object      |             1036 |
| latitude             | float64     |                0 |
| longitude            | float64     |                0 |
| bid                  | object      |            27311 |
| start_date           | object      |                0 |
| end_date             | object      |                0 |
| objectid             | int64       |                0 |
| octo_record_id       | float64     |            33922 |


Looking at the table above, I notice there are three columns ('variables') where they should be dates, but they are listed as objects. I am going to change the data types of those variables using to_datetime so we can extract some more information such as month, year, time duration between start and end date. To do that I wrote the following to create the desired outcome.

```python
# List of variables to change type to date.
dates = ['report_dat', 'start_date', 'end_date']

# For loop changing the varibles data types.
for date in dates:
    df[date] = pd.to_datetime(df[date])

# Creating variables for possible exploration from the dates such as month, year, and time_duration.
df['start_date_one'] = df['start_date']
df['month'] = df['start_date_one'].dt.month
df['year'] = df['start_date_one'].dt.year
df['start_date'] = df['start_date_one'].dt.date
df['start_time'] = df['start_date_one'].dt.time

df['end_date_one'] = df['end_date']
df['end_date'] = df['end_date_one'].dt.date
df['end_time'] = df['end_date_one'].dt.time

df['time_duration'] = df['end_date_one'] - df['start_date_one']
```

Now I really want to start cleaning this df up by deleting rows that don't make sense with the data, addressing missing values, and deleting variables I am not interested in for this general overall data analysis. Whatever variable we do delete, at a later time we can address possibly with other means for data exploration.

For now I am going to stick with just 2019 so I am going to focus on the variable 'year' to ensure we have nothing but 2019 as well as 'time_duration' to ensure those results are positive.

```python
# Checks the counts for each year. We only want 2019 in our case.
df['year'].value_counts()
```

| Year |  Count |
|:----:|:------:|
| 2019 |  33712 |
| 2018 |    190 |
| 2017 |      7 |
| 2020 |      6 |
| 2016 |      5 |
| 2012 |      1 |
| 2005 |      1 |


```python
# Checks to see if we have any negative values for 'time_duration'.
max(df[df['time_duration'] < '0 days 00:00:00'].count())
```

When we run the above pieces of code we have other years listed and for time_duration we have 4,807 rows that are negative. Now that we have that, we will start to delete these rows which we will do that with the following:

```python
# Deletes all rows that are not 2019.
indexyear = df[df['year'] != 2019].index
df.drop(indexyear, inplace=True)

# Deletes all rows with a negative 'time_duration' value.
indextime = df[df['time_duration'] < '0 days 00:00:00'].index
df.drop(indextime, inplace=True)

df['year'].value_counts()
print(max(df[df['time_duration'] < '0 days 00:00:00'].count()))
```

| Year | Count  |
|:----:|:------:|
| 2019 |  28908 |
0

Awesome, we get only 2019 for the year to focus on as well as 0 negative 'time_duration' values which is what we want. Now in the interest of time, I am going to delete some variables that don't make sense to me now that we can always go back later to investigate. I want to limit the variables, fill in the missing values, and start seeing the areas where offenses are high.

I use the following to delete the variables I will not be using in this data analysis:

```python
# Drops the variables from the df we are not going to use.
df.drop(['x', 'y', 'xblock', 'yblock', 'ccn','anc', 'psa', 'block_group', 'bid','census_tract', 'objectid', 
         'octo_record_id', 'end_date_one', 'start_date_one', 'report_dat'], 
        axis=1, inplace=True)
```

So then I rerun our summarize_dataframe function we get the following which I am still just looking at three columns:

```python
summarize_dataframe(df)
```

|       Variables      | Data Type       |   Missing Values |
|:--------------------:|:---------------:|:----------------:|
| shift                | object          |                0 |
| method               | object          |                0 |
| offense              | object          |                0 |
| block                | object          |                0 |
| ward                 | float64         |              904 |
| district             | float64         |                2 |
| neighborhood_cluster | object          |              904 |
| voting_precinct      | object          |              904 |
| latitude             | float64         |                0 |
| longitude            | float64         |                0 |
| start_date           | object          |                0 |
| end_date             | object          |                0 |
| month                | int64           |                0 |
| year                 | int64           |                0 |
| start_time           | object          |                0 |
| end_time             | object          |                0 |
| time_duration        | timedelta64[ns] |                0 |

The above table shows we have four variables with missing data: 1) ward, 2) district, 3) neighborhood_cluster, and 4) voting_precinct. 

```python
df.shape
```
The above code gives us 28,908 rows in our df, so 904/28,908 is a little of 3% missing data for the three variables. In this case for exploratory data analysis, I am comfortable using the most common values for each variable to fill in the missing values. To do that we use the following and then rerun the summarize_dataframe function to ensure we are good:

```python
# Filling in missing values from the columns we have selected. Filling the missing values in with the most used.
df['ward'] = df['ward'].fillna(df['ward'].value_counts().index[0])
df['neighborhood_cluster'] = df['neighborhood_cluster'].fillna(df['neighborhood_cluster'].value_counts().index[0])
df['voting_precinct'] = df['voting_precinct'].fillna(df['voting_precinct'].value_counts().index[0])
df['district'] = df['district'].fillna(df['district'].value_counts().index[0])

summarize_dataframe(df)
```
|     Variables        | Data Type       |   Missing Values |
|:--------------------:|:---------------:|:----------------:|
| shift                | object          |                0 |
| method               | object          |                0 |
| offense              | object          |                0 |
| block                | object          |                0 |
| ward                 | float64         |                0 |
| district             | float64         |                0 |
| neighborhood_cluster | object          |                0 |
| voting_precinct      | object          |                0 |
| latitude             | float64         |                0 |
| longitude            | float64         |                0 |
| start_date           | object          |                0 |
| end_date             | object          |                0 |
| month                | int64           |                0 |
| year                 | int64           |                0 |
| start_time           | object          |                0 |
| end_time             | object          |                0 |
| time_duration        | timedelta64[ns] |                0 |


Now we want to start looking at some of this data through various charts so we can start to get an idea what is going on here. First, since we have variables for longitude and latitude, I want to create a scatter plot to see if we get a map of DC.

![alt text](/img/posts/dc_plot.png "Scatter plot of DC")

Ok so this is definitely a scatter plot showing DC. If you don't believe me then you can certainly do an image search of DC to see for yourself. There is two noticable gaps in the scatter plot which the one on the left is a park area and the gap on the right is a river so it is a pretty good rendering of DC. 

Now lets see what kind of offenses are being tracked in DC with the data we are working with. For this I just created a simple horizontal bar chart to give us an idea.

```python
# I want to to know the different types of offenses and how they stack up against each other.
(df
    .offense.value_counts(ascending=True)
    .plot.barh(color='deeppink', figsize=(10,8)))
```

![alt text](/img/posts/offense_barh.png "Number of Offenses by Type")

So I believe it is safe to say that theft is an issue in DC for 2019. Next, lets see what district/ward has the most so we are going to render the same charts as before but with ward/district.

```python
# I want to to know the different types of wards and how they stack up against each other with repect to (WRT) offenses.
(df
    .ward.value_counts(ascending=True)
    .plot.barh(color='deeppink', figsize=(10,8)))

# I want to to know the different types of districts and how they stack up against each other WRT offenses.
(df
    .district.value_counts(ascending=True)
    .plot.barh(color='deeppink', figsize=(10,8)))
```

![alt text](/img/posts/ward_barh.png "Number of Offenses by Ward")

![alt text](/img/posts/district_barh.png "Number of Offenses by District")

From these charts it looks like ward 2/6 and district 2/3 is where the most offenses took place in 2019 for DC. Out of curiosity lets check the shift variable when these offenses occured. I am not 100% sure the time cut offs between the shifts but in the python code below, I make an educated guess.

```python
# Time periods the offenses are committed. Not sure exactly what the time difference is between the three. Maybe the following:
# Evening: 4pm to 12am
# Midnight: 12am to 8am
# Day: 8am to 4pm
(df['shift']
    .value_counts(ascending=True)
    .plot.barh(color='deeppink', figsize=(10,8)))
```

![alt text](/img/posts/shift_barh.png "Number of Offenses by Shift")


Now we can start to use some scatter plots to see the most offense reported areas by ward/district and the least to see how they compare. To do that we created a python dictionary for wards and districts data which helps us create the plots.

```python
wards = {}
for ward in df.ward.value_counts().index:
    result = 'ward' + '_' + str(ward)
    wards[result] = df[df['ward'] == ward]

districts = {}
for district in df.district.value_counts().index:
    result = 'district' + '_' + str(district)
    districts[result] = df[df['district'] == district]
```

We have the information we want so now we use the following code to plot:

```python
# Next I wanted to see the top crime areas and how they show in the DC area. This plot shows most being internal (central) to DC area
plt.title('Most Crime Instances by Ward(2/6)/District(2/3)')
plt.rcParams.update({'figure.figsize':(8,8), 'figure.dpi': 60})
plt.scatter(x=wards['ward_2'].longitude, y=wards['ward_2'].latitude,
            label='ward_2', s=wards['ward_2'].offense.count()/100, c='purple', alpha=0.05)

plt.scatter(x=wards['ward_6'].longitude, y=wards['ward_6'].latitude,
            label='ward_6', s=wards['ward_6'].offense.count()/100, c='red', alpha=0.05)

plt.scatter(x=districts['district_2'].longitude, y=districts['district_2'].latitude,
            label='district_2', s=districts['district_2'].offense.count()/100, c='blue', alpha=0.01)

plt.scatter(x=districts['district_3'].longitude, y=districts['district_3'].latitude,
            label='district_3', s=districts['district_3'].offense.count()/100, c='green', alpha=0.01)

plt.axis('off')
leg = plt.legend(frameon=False)
for lh in leg.legendHandles:
    lh.set_alpha(1)
plt.tight_layout()
plt.show()
```

![alt text](/img/posts/ward_dist_scat.png "Number of Offenses by Ward (2/6) and District (2/3)")

And then the least incident areas reported:

```python
# If the most crime is reported in central DC then I wanted to see where the least crime is reported. This plot shows what is expected, 
# the out skirts of central DC.
plt.title('Least Crime Instances by Ward')
plt.rcParams.update({'figure.figsize':(8,8), 'figure.dpi': 60})
plt.scatter(x=wards['ward_7'].longitude, y=wards['ward_7'].latitude,
            label='ward_7', s=wards['ward_7'].offense.count()/100, c='blue', alpha=0.05)

plt.scatter(x=wards['ward_4'].longitude, y=wards['ward_4'].latitude,
            label='ward_4', s=wards['ward_4'].offense.count()/100, c='green', alpha=0.05)

plt.scatter(x=wards['ward_8'].longitude, y=wards['ward_8'].latitude,
            label='ward_8', s=wards['ward_8'].offense.count()/100, c='orange', alpha=0.05)

plt.scatter(x=wards['ward_3'].longitude, y=wards['ward_3'].latitude,
            label='ward_3', s=wards['ward_3'].offense.count()/100, c='purple', alpha=0.05)

leg = plt.legend(frameon=False)
for lh in leg.legendHandles:
    lh.set_alpha(1)
plt.axis('off')
plt.tight_layout()
plt.show()
```

![alt text](/img/posts/ward_least_scat.png "Least Number of Offenses by Ward")

As we can see the inner part of DC has high incidents of offenses while the outskirts of DC are less. So one of the last items I wanted to look at is how many offenses occur/reported for ward 2, district 2.

```python
# I take the top value of districts and wards and divide that against the overall to see pecentage wise. We get about 46% of crime from these two
# areas of District 2 and Ward 2.
crime = (df.district.value_counts().values[0] + df.ward.value_counts().values[0])
crime/df.shape[0]

0.4561367095613671

# To see this a little better, this is a crosstab of the information to see percentages by ward and district.
# If you add the ward 2 column of 23.47% with district 2 row of 22.14 you get 45.61% or about 46%.
pd.crosstab(df.district, df.ward, values=df.offense, aggfunc='count', normalize='all', margins=True).fillna(0).round(4)*100
```

| ward/district |     1 |     2 |    3 |    4 |     5 |     6 |     7 |    8 |    All |
|:----------:|:-----:|:-----:|:----:|:----:|:-----:|:-----:|:-----:|:----:|:------:|
| 1          |  0    |  2.6  | 0    | 0    |  0    | 12.02 |  0.08 | 0    |  14.7  |
| 2          |  0    | 16.11 | 5.68 | 0.35 |  0    |  0    |  0    | 0    |  22.14 |
| 3          | 11.98 |  3.7  | 0    | 0    |  1.56 |  1.87 |  0    | 0    |  19.11 |
| 4          |  2.01 |  0.13 | 0    | 7.99 |  1.35 |  0    |  0    | 0    |  11.48 |
| 5          |  0    |  0.34 | 0    | 0    | 11.69 |  2.34 |  0.35 | 0    |  14.72 |
| 6          |  0    |  0.36 | 0    | 0    |  0    |  0    |  9.89 | 1.24 |  11.48 |
| 7          |  0    |  0.24 | 0    | 0    |  0    |  0    |  0    | 6.13 |   6.36 |
| All        | 14    | 23.47 | 5.68 | 8.34 | 14.6  | 16.23 | 10.32 | 7.37 | 100    |

To help better visulize this we use a heatmap in with following code followed by the output:

```python
# This is a heatmap that shows a little more clearly the exact area in DC that has the most crime areas reported.
crosstbl = (pd.crosstab(df
                        .district, df
                        .ward, values=df.offense, aggfunc='count', normalize='all')
                        .fillna(0)
                        .round(4)*100)

sns.heatmap(crosstbl, annot=True, fmt='.2f', linewidth=1, cmap='crest')
```

![alt text](/img/posts/ward_dist_heatmap.png "Heatmap by Ward/District of Offenses")

## Conclusion: ##

My take away from this experience:
1) Seems the areas with most of the crime reported is in the inner part of DC close to the National Mall area.

2) This data primary shows 'Theft' being the offense that is taking place.

3) With this information, you can focus more deterent options in these areas vice lighter areas so resourses are being used appropriately.

4) I would also put public notifications to educate the public about the issues pertaining to this area which is a high tourist area.

What I would do next with this data:

1) I would look more into the dates when these crimes started/reported. This would allow me to see if there are common trends when it comes to month and/or time of day when when crime occured. With this information, we could possibly put more law enforcement in the area during these time windows as a potential deterent.

2) I would string multiple years together to see what the trend of crimes have been in a 5 year period and show an evolution. This could possibly tie with number 1 to develop Key Indicatiors (KI) to put law enforcement focus and to see if our action(s) is/are having the desired result.


There is a lot that can be done with this simple data set and this is just one approach that can be built on further!